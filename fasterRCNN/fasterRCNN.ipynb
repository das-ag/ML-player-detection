{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb96af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasterrcnn_pretrained_test.ipynb\n",
    "# Purpose: Use pretrained Faster R-CNN to detect players on SoccerNet dataset frames\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- CONFIG ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BASE_DATA_DIR = \"../soccernet_data/tracking/test\"\n",
    "GT_FILENAME = \"gt.txt\"\n",
    "IMAGE_FOLDER = \"img1\"\n",
    "IMAGE_EXTS = ['.jpg', '.png']\n",
    "NUM_IMAGES = 10\n",
    "SCORE_THRESH = 0.8\n",
    "IOU_THRESH = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b296618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- UTILS -----\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "# --- IMAGE PREPROCESSING ---\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "def load_gt_boxes(gt_path):\n",
    "    gt_dict = defaultdict(list)\n",
    "    with open(gt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            frame, _, x, y, w, h, cls, _, _ = map(int, parts[:9])\n",
    "            box = torch.tensor([x, y, x + w, y + h], device=DEVICE)\n",
    "            gt_dict[frame].append(box)\n",
    "    return gt_dict\n",
    "\n",
    "# --- IOU CALCULATION ---\n",
    "def compute_iou(box1, box2):\n",
    "    # box1: [N, 4], box2: [M, 4]\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "\n",
    "    lt = torch.max(box1[:, None, :2], box2[:, :2])\n",
    "    rb = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    union = area1[:, None] + area2 - inter\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "# --- DRAW DETECTIONS + GT ---\n",
    "def plot_gt_and_detections(image_tensor, detections, gt_boxes, score_thresh=0.8):\n",
    "    boxes = detections['boxes']\n",
    "    scores = detections['scores']\n",
    "    keep = scores > score_thresh\n",
    "    pred_boxes = boxes[keep]\n",
    "\n",
    "    all_boxes = []\n",
    "    labels = []\n",
    "    colors = []\n",
    "\n",
    "    for box in pred_boxes:\n",
    "        all_boxes.append(box)\n",
    "        labels.append(\"pred\")\n",
    "        colors.append(\"red\")\n",
    "\n",
    "    for box in gt_boxes:\n",
    "        all_boxes.append(box)\n",
    "        labels.append(\"gt\")\n",
    "        colors.append(\"green\")\n",
    "\n",
    "    if len(all_boxes) == 0:\n",
    "        return T.ToPILImage()(image_tensor)\n",
    "\n",
    "    all_boxes_tensor = torch.stack(all_boxes).to(\"cpu\")\n",
    "    drawn = draw_bounding_boxes(\n",
    "        (image_tensor * 255).byte().cpu(),\n",
    "        boxes=all_boxes_tensor,\n",
    "        labels=labels,\n",
    "        colors=colors,\n",
    "        width=2\n",
    "    )\n",
    "    return T.ToPILImage()(drawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643291f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:38<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m gt_boxes \u001b[38;5;241m=\u001b[39m gt_dict\u001b[38;5;241m.\u001b[39mget(frame_id, [])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m][output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m SCORE_THRESH]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gt_boxes:\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:366\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    364\u001b[0m num_anchors_per_level_shape_tensors \u001b[38;5;241m=\u001b[39m [o[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m objectness]\n\u001b[1;32m    365\u001b[0m num_anchors_per_level \u001b[38;5;241m=\u001b[39m [s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m s[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m s[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m num_anchors_per_level_shape_tensors]\n\u001b[0;32m--> 366\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_box_prediction_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjectness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[1;32m    370\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mdecode(pred_bbox_deltas\u001b[38;5;241m.\u001b[39mdetach(), anchors)\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:100\u001b[0m, in \u001b[0;36mconcat_box_prediction_layers\u001b[0;34m(box_cls, box_regression)\u001b[0m\n\u001b[1;32m     98\u001b[0m A \u001b[38;5;241m=\u001b[39m Ax4 \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     99\u001b[0m C \u001b[38;5;241m=\u001b[39m AxC \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m A\n\u001b[0;32m--> 100\u001b[0m box_cls_per_level \u001b[38;5;241m=\u001b[39m \u001b[43mpermute_and_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox_cls_per_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m box_cls_flattened\u001b[38;5;241m.\u001b[39mappend(box_cls_per_level)\n\u001b[1;32m    103\u001b[0m box_regression_per_level \u001b[38;5;241m=\u001b[39m permute_and_flatten(box_regression_per_level, N, A, \u001b[38;5;241m4\u001b[39m, H, W)\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:84\u001b[0m, in \u001b[0;36mpermute_and_flatten\u001b[0;34m(layer, N, A, C, H, W)\u001b[0m\n\u001b[1;32m     82\u001b[0m layer \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mview(N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C, H, W)\n\u001b[1;32m     83\u001b[0m layer \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layer\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- EXECUTION ---\n",
    "results = []\n",
    "sample_frames = []\n",
    "seq_dirs = sorted([d for d in os.listdir(BASE_DATA_DIR) if os.path.isdir(os.path.join(BASE_DATA_DIR, d))])\n",
    "\n",
    "for seq_id in tqdm(seq_dirs):\n",
    "    seq_path = os.path.join(BASE_DATA_DIR, seq_id)\n",
    "    img_dir = os.path.join(seq_path, IMAGE_FOLDER)\n",
    "    gt_path = os.path.join(seq_path, \"gt\", GT_FILENAME)\n",
    "    gt_dict = load_gt_boxes(gt_path)\n",
    "\n",
    "    all_img_paths = sorted([\n",
    "        os.path.join(img_dir, file)\n",
    "        for file in os.listdir(img_dir)\n",
    "        if any(file.lower().endswith(ext) for ext in IMAGE_EXTS)\n",
    "    ])\n",
    "\n",
    "    for idx, path in enumerate(all_img_paths):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "        filename = os.path.basename(path)\n",
    "        frame_id = int(filename.split('.')[0])\n",
    "        gt_boxes = gt_dict.get(frame_id, [])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)[0]\n",
    "\n",
    "        pred_boxes = output['boxes'][output['scores'] > SCORE_THRESH]\n",
    "        if gt_boxes:\n",
    "            gt_tensor = torch.stack(gt_boxes)\n",
    "            if len(pred_boxes) > 0:\n",
    "                ious = compute_iou(pred_boxes, gt_tensor)\n",
    "                max_ious = ious.max(dim=1)[0]\n",
    "                acc = (max_ious > IOU_THRESH).float().mean().item()\n",
    "            else:\n",
    "                acc = 0.0\n",
    "        else:\n",
    "            acc = 1.0 if len(pred_boxes) == 0 else 0.0\n",
    "\n",
    "        results.append(acc)\n",
    "\n",
    "        # Save one visual example per sequence\n",
    "        if idx == 0 and len(sample_frames) < NUM_IMAGES:\n",
    "            img_vis = plot_gt_and_detections(img_tensor[0], output, gt_boxes)\n",
    "            sample_frames.append((seq_id, filename, img_vis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- DISPLAY SUMMARY ---\n",
    "avg_acc = sum(results) / len(results) if results else 0\n",
    "print(f\"\\nAverage Detection Accuracy over {len(results)} frames: {avg_acc * 100:.2f}%\")\n",
    "\n",
    "# --- SHOW EXAMPLES ---\n",
    "for seq_id, filename, img in sample_frames:\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Sequence {seq_id}, Frame {filename}\\nRed = Prediction, Green = Ground Truth\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
