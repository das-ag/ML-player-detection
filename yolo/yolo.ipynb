{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e952f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46abf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequences:   0%|          | 0/106 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 24 persons, 1 sports ball, 217.0ms\n",
      "Speed: 2.4ms preprocess, 217.0ms inference, 95.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequences:   0%|          | 0/106 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pass individual images, not batches",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 180\u001b[0m\n\u001b[1;32m    177\u001b[0m         total_fn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample_frames) \u001b[38;5;241m<\u001b[39m NUM_VISUALS \u001b[38;5;129;01mand\u001b[39;00m seq_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sample_frames]:\n\u001b[0;32m--> 180\u001b[0m             img_vis \u001b[38;5;241m=\u001b[39m \u001b[43mplot_gt_and_detections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m             sample_frames\u001b[38;5;241m.\u001b[39mappend((seq_id, filename, img_vis))\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# --- METRICS ---\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 78\u001b[0m, in \u001b[0;36mplot_gt_and_detections\u001b[0;34m(image_tensor, detections, gt_boxes)\u001b[0m\n\u001b[1;32m     76\u001b[0m boxes_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(all_boxes)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m img_uint8 \u001b[38;5;241m=\u001b[39m (image_tensor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m---> 78\u001b[0m drawn \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_uint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m T\u001b[38;5;241m.\u001b[39mToPILImage()(drawn)\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/practice/lib/python3.10/site-packages/torchvision/utils.py:198\u001b[0m, in \u001b[0;36mdraw_bounding_boxes\u001b[0;34m(image, boxes, labels, colors, fill, width, font, font_size)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor uint8 expected, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass individual images, not batches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m}:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly grayscale and RGB images are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Pass individual images, not batches"
     ]
    }
   ],
   "source": [
    "# yolov8_pretrained_test.py\n",
    "# Purpose: Use pretrained YOLOv8-nano to detect players on SoccerNet dataset frames\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO  # Ensure you have `pip install ultralytics`\n",
    "import random\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BASE_DATA_ROOT = \"../soccernet_data/tracking\"\n",
    "GT_FILENAME = \"gt.txt\"\n",
    "IMAGE_FOLDER = \"img1\"\n",
    "IMAGE_EXTS = ['.jpg', '.png']\n",
    "NUM_VISUALS = 10\n",
    "SCORE_THRESH = 0.5\n",
    "IOU_THRESH = 0.5\n",
    "SAMPLE_PER_SEQ = 30\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "model = YOLO(\"yolov8n.pt\")  # yolov8 nano model\n",
    "model.half()\n",
    "model.to(DEVICE)\n",
    "\n",
    "transform = T.ToTensor()\n",
    "\n",
    "def load_gt_boxes(gt_path):\n",
    "    gt_dict = defaultdict(list)\n",
    "    if not os.path.exists(gt_path):\n",
    "        return gt_dict\n",
    "    with open(gt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            frame, _, x, y, w, h, cls, _, _ = map(int, parts[:9])\n",
    "            gt_dict[frame].append(torch.tensor([x, y, x + w, y + h], device=DEVICE))\n",
    "    return gt_dict\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    if box1.size(0) == 0 or box2.size(0) == 0:\n",
    "        return torch.zeros((box1.size(0), box2.size(0)), device=box1.device)\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "    lt = torch.max(box1[:, None, :2], box2[:, :2])\n",
    "    rb = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    union = area1[:, None] + area2 - inter\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "def plot_gt_and_detections(image_tensor, detections, gt_boxes):\n",
    "    from torchvision.utils import draw_bounding_boxes\n",
    "    all_boxes = []\n",
    "    labels = []\n",
    "    colors = []\n",
    "\n",
    "    for box in detections:\n",
    "        all_boxes.append(box)\n",
    "        labels.append(\"pred\")\n",
    "        colors.append(\"red\")\n",
    "\n",
    "    for box in gt_boxes:\n",
    "        all_boxes.append(box)\n",
    "        labels.append(\"gt\")\n",
    "        colors.append(\"green\")\n",
    "\n",
    "    if not all_boxes:\n",
    "        return T.ToPILImage()(image_tensor)\n",
    "\n",
    "    boxes_tensor = torch.stack(all_boxes).to(torch.uint8).to(\"cpu\")\n",
    "    img_uint8 = (image_tensor * 255).byte().cpu()\n",
    "    drawn = draw_bounding_boxes(img_uint8, boxes_tensor, labels=labels, colors=colors, width=2)\n",
    "    return T.ToPILImage()(drawn)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "results = []\n",
    "sample_frames = []\n",
    "total_tp = total_fp = total_fn = 0\n",
    "\n",
    "seq_dirs = []\n",
    "for split in [\"train\", \"test\"]:\n",
    "    split_dir = os.path.join(BASE_DATA_ROOT, split)\n",
    "    if not os.path.exists(split_dir):\n",
    "        continue\n",
    "    for d in sorted(os.listdir(split_dir)):\n",
    "        full_path = os.path.join(split_dir, d)\n",
    "        if os.path.isdir(full_path):\n",
    "            seq_dirs.append((split, d))\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"Processing sequences...\")\n",
    "\n",
    "for split, seq_id in tqdm(seq_dirs, desc=\"Sequences\", dynamic_ncols=True):\n",
    "    seq_path = os.path.join(BASE_DATA_ROOT, split, seq_id)\n",
    "    img_dir = os.path.join(seq_path, IMAGE_FOLDER)\n",
    "    gt_path = os.path.join(seq_path, \"gt\", GT_FILENAME)\n",
    "    gt_dict = load_gt_boxes(gt_path)\n",
    "\n",
    "    if not os.path.exists(img_dir):\n",
    "        continue\n",
    "\n",
    "    all_img_paths = sorted([\n",
    "        os.path.join(img_dir, file)\n",
    "        for file in os.listdir(img_dir)\n",
    "        if any(file.lower().endswith(ext) for ext in IMAGE_EXTS)\n",
    "    ])\n",
    "\n",
    "    random.shuffle(all_img_paths)\n",
    "    all_img_paths = all_img_paths[:SAMPLE_PER_SEQ]\n",
    "\n",
    "    for path in all_img_paths:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "        img_tensor = img_tensor.half()\n",
    "        img_tensor = img_tensor.squeeze(0) \n",
    "\n",
    "        img_tensor = img_tensor.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        filename = os.path.basename(path)\n",
    "        try:\n",
    "            frame_id = int(filename.split('.')[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        gt_boxes = gt_dict.get(frame_id, [])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            yolo_result = model(img)[0]  # Get first result\n",
    "            preds = yolo_result.boxes.data.to(DEVICE) if yolo_result.boxes is not None else torch.empty((0, 6)).to(DEVICE)\n",
    "\n",
    "        pred_boxes = preds[:, :4][preds[:, 4] > SCORE_THRESH] if len(preds) else torch.empty((0, 4), device=DEVICE)\n",
    "\n",
    "        # Accuracy\n",
    "        if gt_boxes:\n",
    "            gt_tensor = torch.stack(gt_boxes).to(DEVICE)\n",
    "            if len(pred_boxes) > 0:\n",
    "                ious = compute_iou(pred_boxes, gt_tensor)\n",
    "                max_ious = ious.max(dim=1)[0]\n",
    "                acc = (max_ious > IOU_THRESH).float().mean().item()\n",
    "            else:\n",
    "                acc = 0.0\n",
    "        else:\n",
    "            acc = 1.0 if len(pred_boxes) == 0 else 0.0\n",
    "\n",
    "        results.append(acc)\n",
    "\n",
    "        # Precision/Recall\n",
    "        matched_gt = set()\n",
    "        tp = fp = 0\n",
    "        if len(pred_boxes) > 0 and len(gt_boxes) > 0:\n",
    "            ious = compute_iou(pred_boxes, gt_tensor)\n",
    "            for i in range(len(pred_boxes)):\n",
    "                max_iou, gt_idx = ious[i].max(0)\n",
    "                if max_iou > IOU_THRESH and gt_idx.item() not in matched_gt:\n",
    "                    tp += 1\n",
    "                    matched_gt.add(gt_idx.item())\n",
    "                else:\n",
    "                    fp += 1\n",
    "        else:\n",
    "            tp = 0\n",
    "            fp = len(pred_boxes)\n",
    "\n",
    "        fn = len(gt_boxes) - len(matched_gt)\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "        if len(sample_frames) < NUM_VISUALS and seq_id not in [s[0] for s in sample_frames]:\n",
    "            img_vis = plot_gt_and_detections(img_tensor, pred_boxes, gt_boxes)\n",
    "            sample_frames.append((seq_id, filename, img_vis))\n",
    "            \n",
    "\n",
    "# --- METRICS ---\n",
    "precision = total_tp / (total_tp + total_fp + 1e-6)\n",
    "recall = total_tp / (total_tp + total_fn + 1e-6)\n",
    "avg_acc = sum(results) / len(results) if results else 0\n",
    "print(f\"\\nAverage Detection Accuracy over {len(results)} frames: {avg_acc * 100:.2f}%\")\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}\")\n",
    "\n",
    "# --- SHOW EXAMPLES ---\n",
    "for seq_id, filename, img in sample_frames:\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Sequence {seq_id}, Frame {filename}\\nRed = Prediction, Green = Ground Truth\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bc702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
