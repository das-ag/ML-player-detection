{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e952f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO  \n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import random\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\") \n",
    "BASE_DATA_ROOT = \"../soccernet_data/tracking\"\n",
    "GT_FILENAME = \"gt.txt\"\n",
    "IMAGE_FOLDER = \"img1\"\n",
    "IMAGE_EXTS = ['.jpg', '.png']\n",
    "NUM_VISUALS = 10\n",
    "SCORE_THRESH = 0.8\n",
    "IOU_THRESH = 0.5\n",
    "SAMPLE_PER_SEQ = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83db7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- LOAD MODEL ---\n",
    "model_yolo = YOLO(\"yolo11n.pt\") \n",
    "model_yolo.to(DEVICE)\n",
    "\n",
    "model_frcnn = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model_frcnn.to(DEVICE)\n",
    "\n",
    "\n",
    "transform = T.ToTensor()\n",
    "\n",
    "def load_gt_boxes(gt_path):\n",
    "    gt_dict = defaultdict(list)\n",
    "    if not os.path.exists(gt_path):\n",
    "        return gt_dict\n",
    "    with open(gt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            frame, _, x, y, w, h, cls, _, _ = map(int, parts[:9])\n",
    "            gt_dict[frame].append(torch.tensor([x, y, x + w, y + h], device=DEVICE))\n",
    "    return gt_dict\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    if box1.size(0) == 0 or box2.size(0) == 0:\n",
    "        return torch.zeros((box1.size(0), box2.size(0)), device=box1.device)\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "    lt = torch.max(box1[:, None, :2], box2[:, :2])\n",
    "    rb = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    union = area1[:, None] + area2 - inter\n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "def plot_gt_and_detections(image_tensor, detections, gt_boxes):\n",
    "    from torchvision.utils import draw_bounding_boxes\n",
    "    all_boxes = []\n",
    "    labels = []\n",
    "    colors = []\n",
    "\n",
    "    for box in detections:\n",
    "        all_boxes.append(box)\n",
    "        labels.append(\"pred\")\n",
    "        colors.append(\"red\")\n",
    "\n",
    "    for box in gt_boxes:\n",
    "        all_boxes.append(box)\n",
    "        labels.append(\"gt\")\n",
    "        colors.append(\"green\")\n",
    "\n",
    "    if not all_boxes:\n",
    "        return T.ToPILImage()(image_tensor)\n",
    "    \n",
    "    boxes_tensor = torch.stack(all_boxes).cpu()\n",
    "\n",
    "    \n",
    "    x1 = torch.min(boxes_tensor[:, 0], boxes_tensor[:, 2])\n",
    "    y1 = torch.min(boxes_tensor[:, 1], boxes_tensor[:, 3])\n",
    "    x2 = torch.max(boxes_tensor[:, 0], boxes_tensor[:, 2])\n",
    "    y2 = torch.max(boxes_tensor[:, 1], boxes_tensor[:, 3])\n",
    "    boxes_tensor = torch.stack([x1, y1, x2, y2], dim=1).to(torch.int)\n",
    "    img_uint8 = (image_tensor * 255).byte().cpu()\n",
    "    drawn = draw_bounding_boxes(img_uint8, boxes_tensor, labels=labels, colors=colors, width=2)\n",
    "    return T.ToPILImage()(drawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46abf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Processing sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequences:   0%|          | 0/106 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 18 persons, 44.1ms\n",
      "Speed: 1.9ms preprocess, 44.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: command buffer exited with error status.\n",
      "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
      "\tError: \n",
      "\t(null)\n",
      "\tInternal Error (0000000e:Internal Error)\n",
      "\t<AGXG13XFamilyCommandBuffer: 0x385d6ab60>\n",
      "    label = <none> \n",
      "    device = <AGXG13XDevice: 0x145729800>\n",
      "        name = Apple M1 Max \n",
      "    commandQueue = <AGXG13XFamilyCommandQueue: 0x1065a6e00>\n",
      "        label = <none> \n",
      "        device = <AGXG13XDevice: 0x145729800>\n",
      "            name = Apple M1 Max \n",
      "    retainedReferences = 1\n",
      "Sequences:   0%|          | 0/106 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# RUN FRCNN\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 127\u001b[0m     fr_out \u001b[38;5;241m=\u001b[39m model_frcnn([img_frcnn])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    128\u001b[0m fr_scores \u001b[38;5;241m=\u001b[39m fr_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    129\u001b[0m fr_boxes  \u001b[38;5;241m=\u001b[39m fr_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torchvision/models/detection/generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m    104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m--> 105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torchvision/models/detection/roi_heads.py:761\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    758\u001b[0m     regression_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_roi_pool(features, proposals, image_shapes)\n\u001b[1;32m    762\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_head(box_features)\n\u001b[1;32m    763\u001b[0m class_logits, box_regression \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_predictor(box_features)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torchvision/ops/poolers.py:314\u001b[0m, in \u001b[0;36mMultiScaleRoIAlign.forward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_levels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_levels \u001b[38;5;241m=\u001b[39m _setup_scales(\n\u001b[1;32m    311\u001b[0m         x_filtered, image_shapes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical_scale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical_level\n\u001b[1;32m    312\u001b[0m     )\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _multiscale_roi_align(\n\u001b[1;32m    315\u001b[0m     x_filtered,\n\u001b[1;32m    316\u001b[0m     boxes,\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size,\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_ratio,\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales,\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_levels,\n\u001b[1;32m    321\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML_FinalProj/lib/python3.11/site-packages/torchvision/ops/poolers.py:201\u001b[0m, in \u001b[0;36m_multiscale_roi_align\u001b[0;34m(x_filtered, boxes, output_size, sampling_ratio, scales, mapper)\u001b[0m\n\u001b[1;32m    199\u001b[0m tracing_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level, (per_level_feature, scale) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(x_filtered, scales)):\n\u001b[0;32m--> 201\u001b[0m     idx_in_level \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(levels \u001b[38;5;241m==\u001b[39m level)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    202\u001b[0m     rois_per_level \u001b[38;5;241m=\u001b[39m rois[idx_in_level]\n\u001b[1;32m    204\u001b[0m     result_idx_in_level \u001b[38;5;241m=\u001b[39m roi_align(\n\u001b[1;32m    205\u001b[0m         per_level_feature,\n\u001b[1;32m    206\u001b[0m         rois_per_level,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m         sampling_ratio\u001b[38;5;241m=\u001b[39msampling_ratio,\n\u001b[1;32m    210\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# --- LOAD MODELS ---\n",
    "model_yolo   = YOLO(\"yolo11n.pt\").to(DEVICE).eval()\n",
    "model_frcnn  = fasterrcnn_resnet50_fpn(pretrained=True).to(DEVICE).eval()\n",
    "\n",
    "transform = T.ToTensor()\n",
    "\n",
    "def load_gt_boxes(gt_path):\n",
    "    \"\"\"\n",
    "    Load ground‐truth boxes and classes per frame.\n",
    "    Returns dict: frame_id -> list of dict {'box':Tensor[x1,y1,x2,y2], 'class':int}\n",
    "    \"\"\"\n",
    "    gt_dict = defaultdict(list)\n",
    "    if not os.path.exists(gt_path):\n",
    "        return gt_dict\n",
    "    with open(gt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            frame, _, x, y, w, h, cls, _, _ = map(int, parts[:9])\n",
    "            box = torch.tensor([x, y, x + w, y + h], device=DEVICE)\n",
    "            gt_dict[frame].append({'box': box, 'class': cls})\n",
    "    return gt_dict\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    if box1.size(0)==0 or box2.size(0)==0:\n",
    "        return torch.zeros((box1.size(0), box2.size(0)), device=box1.device)\n",
    "    area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])\n",
    "    area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])\n",
    "    lt = torch.max(box1[:,None,:2], box2[:,:2])\n",
    "    rb = torch.min(box1[:,None,2:], box2[:,2:])\n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:,:,0] * wh[:,:,1]\n",
    "    union = area1[:,None] + area2 - inter\n",
    "    return inter / (union + 1e-6)\n",
    "\n",
    "def plot_detections(image_tensor, yolo_boxes, frcnn_boxes, gt_entries):\n",
    "    from torchvision.utils import draw_bounding_boxes\n",
    "    all_boxes, labels, colors = [], [], []\n",
    "    # YOLO in purple\n",
    "    for b in yolo_boxes:\n",
    "        all_boxes.append(b); labels.append(\"yolo\");   colors.append(\"purple\")\n",
    "    # FRCNN in blue\n",
    "    for b in frcnn_boxes:\n",
    "        all_boxes.append(b); labels.append(\"frcnn\");  colors.append(\"blue\")\n",
    "    # GT in red\n",
    "    for e in gt_entries:\n",
    "        all_boxes.append(e['box']); labels.append(f\"gt:{e['class']}\"); colors.append(\"red\")\n",
    "    if not all_boxes:\n",
    "        return T.ToPILImage()(image_tensor)\n",
    "    boxes = torch.stack(all_boxes).cpu()\n",
    "    x1 = torch.min(boxes[:,0], boxes[:,2])\n",
    "    y1 = torch.min(boxes[:,1], boxes[:,3])\n",
    "    x2 = torch.max(boxes[:,0], boxes[:,2])\n",
    "    y2 = torch.max(boxes[:,1], boxes[:,3])\n",
    "    boxes_int = torch.stack([x1, y1, x2, y2], dim=1).to(torch.int)\n",
    "    img_uint8 = (image_tensor * 255).byte().cpu()\n",
    "    drawn = draw_bounding_boxes(img_uint8, boxes_int, labels=labels, colors=colors, width=2)\n",
    "    return T.ToPILImage()(drawn)\n",
    "\n",
    "# --- BENCHMARK SETUP ---\n",
    "metrics_yolo  = []\n",
    "metrics_frcnn = []\n",
    "sample_frames = []\n",
    "\n",
    "# gather sequences\n",
    "seq_dirs = []\n",
    "for split in [\"train\",\"test\"]:\n",
    "    root = os.path.join(BASE_DATA_ROOT, split)\n",
    "    if not os.path.isdir(root): continue\n",
    "    for d in sorted(os.listdir(root)):\n",
    "        if os.path.isdir(os.path.join(root,d)):\n",
    "            seq_dirs.append((split, d))\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"Processing sequences...\")\n",
    "\n",
    "# loop over sequences\n",
    "for split, seq_id in tqdm(seq_dirs, desc=\"Sequences\", dynamic_ncols=True):\n",
    "    seq_path = os.path.join(BASE_DATA_ROOT, split, seq_id)\n",
    "    img_dir  = os.path.join(seq_path, IMAGE_FOLDER)\n",
    "    gt_path  = os.path.join(seq_path, \"gt\", GT_FILENAME)\n",
    "    gt_dict  = load_gt_boxes(gt_path)\n",
    "    if not os.path.isdir(img_dir):\n",
    "        continue\n",
    "\n",
    "    img_files = sorted([os.path.join(img_dir,f)\n",
    "                        for f in os.listdir(img_dir)\n",
    "                        if any(f.lower().endswith(ext) for ext in IMAGE_EXTS)])\n",
    "    random.shuffle(img_files)\n",
    "    img_files = img_files[:SAMPLE_PER_SEQ]\n",
    "\n",
    "    # per‐sequence accumulators\n",
    "    accs_y, tp_y, fp_y, fn_y = [], 0,0,0\n",
    "    accs_f, tp_f, fp_f, fn_f = [], 0,0,0\n",
    "    gt_counter = Counter()\n",
    "\n",
    "    for path in img_files:\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "        except:\n",
    "            continue\n",
    "        img_frcnn = transform(img).to(DEVICE)\n",
    "        filename   = os.path.basename(path)\n",
    "        try:\n",
    "            frame_id = int(filename.split('.')[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        gt_entries = gt_dict.get(frame_id, [])\n",
    "        gt_boxes   = [e['box'] for e in gt_entries]\n",
    "        for e in gt_entries:\n",
    "            gt_counter[e['class']] += 1\n",
    "\n",
    "        # RUN YOLO\n",
    "        with torch.no_grad():\n",
    "            yolo_results = model_yolo(img)       # returns list of Results\n",
    "        y_res = yolo_results[0]\n",
    "        preds_y = (y_res.boxes.data.to(DEVICE)\n",
    "                   if getattr(y_res, 'boxes', None) is not None\n",
    "                   else torch.empty((0,6), device=DEVICE))\n",
    "        y_boxes = preds_y[:,:4][preds_y[:,4] > SCORE_THRESH] if preds_y.numel() else torch.empty((0,4), device=DEVICE)\n",
    "\n",
    "        # RUN FRCNN\n",
    "        with torch.no_grad():\n",
    "            fr_out = model_frcnn([img_frcnn])[0]\n",
    "        fr_scores = fr_out['scores']\n",
    "        fr_boxes  = fr_out['boxes'].to(DEVICE)\n",
    "        mask_f    = fr_scores > SCORE_THRESH\n",
    "        f_boxes   = fr_boxes[mask_f]\n",
    "\n",
    "        # ACCURACY YOLO\n",
    "        if gt_boxes:\n",
    "            gt_t = torch.stack(gt_boxes)\n",
    "            if y_boxes.size(0):\n",
    "                ious = compute_iou(y_boxes, gt_t)\n",
    "                m    = ious.max(1)[0]\n",
    "                acc_y = float((m > IOU_THRESH).float().mean())\n",
    "            else:\n",
    "                acc_y = 0.0\n",
    "        else:\n",
    "            acc_y = 1.0 if y_boxes.size(0)==0 else 0.0\n",
    "        accs_y.append(acc_y)\n",
    "\n",
    "        # ACCURACY FRCNN\n",
    "        if gt_boxes:\n",
    "            if f_boxes.size(0):\n",
    "                ious = compute_iou(f_boxes, gt_t)\n",
    "                m    = ious.max(1)[0]\n",
    "                acc_f = float((m > IOU_THRESH).float().mean())\n",
    "            else:\n",
    "                acc_f = 0.0\n",
    "        else:\n",
    "            acc_f = 1.0 if f_boxes.size(0)==0 else 0.0\n",
    "        accs_f.append(acc_f)\n",
    "\n",
    "        # PREC/RECALL YOLO\n",
    "        matched = set(); tp=0; fp=0\n",
    "        if y_boxes.size(0) and gt_boxes:\n",
    "            ious = compute_iou(y_boxes, gt_t)\n",
    "            for i in range(y_boxes.size(0)):\n",
    "                mi, gi = ious[i].max(0)\n",
    "                if mi > IOU_THRESH and gi.item() not in matched:\n",
    "                    tp += 1; matched.add(gi.item())\n",
    "                else:\n",
    "                    fp += 1\n",
    "        else:\n",
    "            fp = y_boxes.size(0)\n",
    "        fn = len(gt_boxes) - len(matched)\n",
    "        tp_y += tp; fp_y += fp; fn_y += fn\n",
    "\n",
    "        # PREC/RECALL FRCNN\n",
    "        matched = set(); tp=0; fp=0\n",
    "        if f_boxes.size(0) and gt_boxes:\n",
    "            ious = compute_iou(f_boxes, gt_t)\n",
    "            for i in range(f_boxes.size(0)):\n",
    "                mi, gi = ious[i].max(0)\n",
    "                if mi > IOU_THRESH and gi.item() not in matched:\n",
    "                    tp += 1; matched.add(gi.item())\n",
    "                else:\n",
    "                    fp += 1\n",
    "        else:\n",
    "            fp = f_boxes.size(0)\n",
    "        fn = len(gt_boxes) - len(matched)\n",
    "        tp_f += tp; fp_f += fp; fn_f += fn\n",
    "\n",
    "        # VISUALIZE A FEW\n",
    "        if len(sample_frames) < NUM_VISUALS and seq_id not in [s[0] for s in sample_frames]:\n",
    "            vis = plot_detections(img_frcnn, y_boxes, f_boxes, gt_entries)\n",
    "            sample_frames.append((seq_id, filename, vis))\n",
    "\n",
    "    # SEQ‐LEVEL METRICS YOLO\n",
    "    avg_y = sum(accs_y)/len(accs_y) if accs_y else 0.0\n",
    "    prec_y = tp_y/(tp_y+fp_y+1e-6)\n",
    "    rec_y  = tp_y/(tp_y+fn_y+1e-6)\n",
    "    metrics_yolo.append({\n",
    "        'split': split, 'seq': seq_id,\n",
    "        'avg_acc': avg_y, 'precision': prec_y, 'recall': rec_y,\n",
    "        'gt_counts': dict(gt_counter)\n",
    "    })\n",
    "\n",
    "    # SEQ‐LEVEL METRICS FRCNN\n",
    "    avg_f = sum(accs_f)/len(accs_f) if accs_f else 0.0\n",
    "    prec_f = tp_f/(tp_f+fp_f+1e-6)\n",
    "    rec_f  = tp_f/(tp_f+fn_f+1e-6)\n",
    "    metrics_frcnn.append({\n",
    "        'split': split, 'seq': seq_id,\n",
    "        'avg_acc': avg_f, 'precision': prec_f, 'recall': rec_f,\n",
    "        'gt_counts': dict(gt_counter)\n",
    "    })\n",
    "\n",
    "# SAVE METRICS TO LOCAL FILES\n",
    "with open(\"metrics_yolo.json\", \"w\") as f:\n",
    "    json.dump(metrics_yolo, f, indent=2)\n",
    "with open(\"metrics_frcnn.json\", \"w\") as f:\n",
    "    json.dump(metrics_frcnn, f, indent=2)\n",
    "\n",
    "# Optionally show some examples\n",
    "for seq_id, filename, img in sample_frames:\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Seq {seq_id}, Frame {filename}\\nPurple=YOLO, Blue=FRCNN, Red=GT\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bc702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_FinalProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
